"""
AI Service - æ‹çˆ±å†›å¸ˆæ ¸å¿ƒé€»è¾‘
"""
import json
import os
from typing import Any, Dict

from dotenv import load_dotenv
from loguru import logger
from openai import AsyncOpenAI
from pydantic import ValidationError
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed

# å¼•å…¥æ–°å®šä¹‰çš„ Schema å’Œ Config
from models.schemas import AdvisorResponse
from config.styles import build_advisor_prompt, get_random_styles

class AIService:
    """
    AI æœåŠ¡ç±» - æ‹çˆ±å†›å¸ˆç‰ˆ
    """
    
    def __init__(self) -> None:
        logger.info("ğŸš€ [AIService] Initializing Dating Advisor Service...")
        self._refresh_config()

    def _refresh_config(self) -> None:
        """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®"""
        load_dotenv(override=True)
        self.api_key = os.getenv("SILICONFLOW_API_KEY", "")
        # æ¨èä½¿ç”¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›å¼ºçš„æ¨¡å‹
        self.model = os.getenv("AI_MODEL", "Qwen/Qwen2.5-72B-Instruct")
        self.max_tokens = int(os.getenv("AI_MAX_TOKENS", "2048"))
        self.temperature = float(os.getenv("AI_TEMPERATURE", "0.95")) # æé«˜åˆ›é€ æ€§
        
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url="https://api.siliconflow.cn/v1"
        )
        
        logger.success(f"âœ… [Config] Model: {self.model} | Temp: {self.temperature}")

    def _parse_response(self, raw_content: str) -> Dict[str, Any]:
        """
        è§£æ LLM è¿”å›çš„ JSON å“åº”å¹¶éªŒè¯æ•°æ®ç»“æ„
        """
        clean_content = raw_content.replace("```json", "").replace("```", "").strip()
        
        logger.debug(f"ğŸ“ [Parse] Raw content length: {len(clean_content)}")
        
        try:
            data = json.loads(clean_content)
            # ä½¿ç”¨æ–°ç‰ˆæ¨¡å‹éªŒè¯
            validated = AdvisorResponse(**data)
            logger.debug(f"âœ… [Validate] Analysis: {validated.analysis[:20]}...")
            return validated.model_dump()
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [Parse] JSON Error: {e}")
            raise e
        except ValidationError as e:
            logger.error(f"âŒ [Parse] Schema Error: {e}")
            raise e

    def _build_context_prompt(self, user_input: str, history: list, selected_styles: list) -> str:
        """
        æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„ Prompt
        
        Args:
            user_input: å¯¹æ–¹æœ€æ–°æ¶ˆæ¯
            history: å†å²å¯¹è¯è®°å½• [{"role": "user", "content": "..."}, ...]
            selected_styles: å·²é€‰æ‹©çš„3ç§é£æ ¼
            
        Returns:
            å®Œæ•´çš„ system prompt
        """
        # 1. æ ¼å¼åŒ–å†å²è®°å½•
        context_str = ""
        if history:
            context_str = "\n# ğŸ“œ Conversation History (Recent Context)\n"
            context_str += "ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç”¨äºç†è§£å½“å‰å±€åŠ¿çš„èƒŒæ™¯ï¼š\n"
            for i, msg in enumerate(history, 1):
                role = "å¯¹æ–¹" if msg.get("role") == "user" else "ä½ ä¹‹å‰çš„å»ºè®®"
                content = msg.get("content", "")
                context_str += f"{i}. {role}: {content}\n"
            context_str += "\n---\n"
        
        # 2. è·å–åŸºç¡€ prompt
        base_prompt = build_advisor_prompt(user_input, selected_styles)
        
        # 3. å°† context æ’å…¥åˆ° Input ä¹‹å‰
        if context_str:
            final_prompt = base_prompt.replace(
                "# Input - The Other Person's Message", 
                f"{context_str}# Input - The Other Person's Message (æœ€æ–°æ¶ˆæ¯)"
            )
        else:
            final_prompt = base_prompt
            
        return final_prompt

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(2),
        retry=retry_if_exception_type((json.JSONDecodeError, ValidationError, Exception)),
        reraise=True,
    )
    async def generate_response(self, user_input: str, history: list = []) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ‹çˆ±å†›å¸ˆå»ºè®®ï¼ˆæ”¯æŒå†å²ä¸Šä¸‹æ–‡ï¼‰
        
        Args:
            user_input: å¯¹æ–¹å‘æ¥çš„æ–‡æœ¬
            history: å†å²å¯¹è¯è®°å½•ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£
            
        Returns:
            AdvisorResponse çš„å­—å…¸å½¢å¼ (analysis, options)
        """
        self._refresh_config()
        
        # 1. éšæœºæŠ½å– 3 ç§é£æ ¼
        selected_styles = get_random_styles(3)
        style_names = [s['name'] for s in selected_styles]
        logger.info(f"ğŸ² [Random] Styles: {style_names} | History Depth: {len(history)}")
        
        # 2. æ„å»ºå¸¦è®°å¿†çš„ Prompt
        system_prompt = self._build_context_prompt(user_input, history, selected_styles)
        
        logger.info(f"âš¡ [Request] Input: {user_input[:30]}... | Context: {len(history)} messages")

        try:
            # 3. è°ƒç”¨ LLM
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    # ä¹Ÿå¯ä»¥é€‰æ‹©æŠŠ user_input æ”¾åœ¨è¿™é‡Œå†æ¬¡å¼ºè°ƒï¼Œæˆ–è€…ä»…é  system prompt
                    {"role": "user", "content": f"å¯¹æ–¹æœ€æ–°æ¶ˆæ¯ï¼š{user_input}"},
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"},
            )

            raw_content = response.choices[0].message.content
            
            # 4. è§£æç»“æœ
            result = self._parse_response(raw_content)
            
            logger.success(f"âœ… [LLM] Generation successful | Options: {len(result.get('options', []))}")
            
            return result
            
        except Exception as exc:
            logger.error(f"âŒ [LLM] Failed: {exc}")
            raise exc

# åˆ›å»ºå…¨å±€å®ä¾‹
ai_service = AIService()