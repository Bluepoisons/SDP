"""
AI Service - æ‹çˆ±å†›å¸ˆæ ¸å¿ƒé€»è¾‘
"""
import json
import os
from typing import Any, Dict

from dotenv import load_dotenv
from loguru import logger
from openai import AsyncOpenAI
from pydantic import ValidationError
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed

# å¼•å…¥æ–°å®šä¹‰çš„ Schema å’Œ Config
from models.schemas import AdvisorResponse
from config.styles import build_advisor_prompt, get_random_styles

class AIService:
    """
    AI æœåŠ¡ç±» - æ‹çˆ±å†›å¸ˆç‰ˆ
    """
    
    def __init__(self) -> None:
        logger.info("ğŸš€ [AIService] Initializing Dating Advisor Service...")
        self._refresh_config()

    def _refresh_config(self) -> None:
        """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®"""
        load_dotenv(override=True)
        self.api_key = os.getenv("SILICONFLOW_API_KEY", "")
        # æ¨èä½¿ç”¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›å¼ºçš„æ¨¡å‹
        self.model = os.getenv("AI_MODEL", "Qwen/Qwen2.5-72B-Instruct")
        self.max_tokens = int(os.getenv("AI_MAX_TOKENS", "2048"))
        self.temperature = float(os.getenv("AI_TEMPERATURE", "0.95")) # æé«˜åˆ›é€ æ€§
        
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url="https://api.siliconflow.cn/v1"
        )
        
        logger.success(f"âœ… [Config] Model: {self.model} | Temp: {self.temperature}")

    def _parse_response(self, raw_content: str) -> Dict[str, Any]:
        """
        è§£æ LLM è¿”å›çš„ JSON å“åº”å¹¶éªŒè¯æ•°æ®ç»“æ„
        """
        clean_content = raw_content.replace("```json", "").replace("```", "").strip()
        
        logger.debug(f"ğŸ“ [Parse] Raw content length: {len(clean_content)}")
        
        try:
            data = json.loads(clean_content)
            # ä½¿ç”¨æ–°ç‰ˆæ¨¡å‹éªŒè¯
            validated = AdvisorResponse(**data)
            logger.debug(f"âœ… [Validate] Analysis: {validated.analysis[:20]}...")
            return validated.model_dump()
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [Parse] JSON Error: {e}")
            raise e
        except ValidationError as e:
            logger.error(f"âŒ [Parse] Schema Error: {e}")
            raise e

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(2),
        retry=retry_if_exception_type((json.JSONDecodeError, ValidationError, Exception)),
        reraise=True,
    )
    async def generate_response(self, user_input: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ‹çˆ±å†›å¸ˆå»ºè®®
        
        Args:
            user_input: å¯¹æ–¹å‘æ¥çš„æ–‡æœ¬
            
        Returns:
            AdvisorResponse çš„å­—å…¸å½¢å¼ (analysis, options)
        """
        self._refresh_config()
        
        # 1. éšæœºæŠ½å– 3 ç§é£æ ¼
        selected_styles = get_random_styles(3)
        style_names = [s['name'] for s in selected_styles]
        logger.info(f"ğŸ² [Random] Selected styles: {style_names}")
        
        # 2. æ„å»º Prompt
        system_prompt = build_advisor_prompt(user_input, selected_styles)
        
        logger.info(f"âš¡ [Request] Input: {user_input[:30]}...")

        try:
            # 3. è°ƒç”¨ LLM
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    # ä¹Ÿå¯ä»¥é€‰æ‹©æŠŠ user_input æ”¾åœ¨è¿™é‡Œå†æ¬¡å¼ºè°ƒï¼Œæˆ–è€…ä»…é  system prompt
                    {"role": "user", "content": f"å¯¹æ–¹å‘æ¥ï¼š{user_input}"},
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"},
            )

            raw_content = response.choices[0].message.content
            
            # 4. è§£æç»“æœ
            result = self._parse_response(raw_content)
            
            logger.success(f"âœ… [LLM] Generation successful | Options: {len(result.get('options', []))}")
            
            return result
            
        except Exception as exc:
            logger.error(f"âŒ [LLM] Failed: {exc}")
            raise exc

# åˆ›å»ºå…¨å±€å®ä¾‹
ai_service = AIService()