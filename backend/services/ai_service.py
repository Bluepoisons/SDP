"""
AI Service - æ‹çˆ±å†›å¸ˆæ ¸å¿ƒé€»è¾‘ v8.0 æŒ‡æŒ¥å®˜ç³»ç»Ÿ
"""
import json
import os
from typing import Any, Dict

from dotenv import load_dotenv
from loguru import logger
from openai import AsyncOpenAI
from pydantic import ValidationError
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed

# å¼•å…¥æ–°å®šä¹‰çš„ Schema å’Œ Config
from models.schemas import AdvisorResponse, SituationAnalysis
from config.styles import (
    build_advisor_prompt, 
    build_analyze_prompt, 
    build_execute_prompt,
    get_random_styles
)

class AIService:
    """
    AI æœåŠ¡ç±» - v8.0 æŒ‡æŒ¥å®˜ç³»ç»Ÿ
    æ”¯æŒåŒé˜¶æ®µå¤„ç†: Analyze (æ€åŠ¿æ„ŸçŸ¥) -> Execute (æˆ˜æœ¯æ‰§è¡Œ)
    """
    
    def __init__(self) -> None:
        logger.info("ğŸš€ [AIService] Initializing Commander System v8.0...")
        self._refresh_config()

    def _refresh_config(self) -> None:
        """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®"""
        load_dotenv(override=True)
        self.api_key = os.getenv("SILICONFLOW_API_KEY", "")
        # æ¨èä½¿ç”¨æŒ‡ä»¤éµå¾ªèƒ½åŠ›å¼ºçš„æ¨¡å‹
        self.model = os.getenv("AI_MODEL", "Qwen/Qwen2.5-72B-Instruct")
        self.max_tokens = int(os.getenv("AI_MAX_TOKENS", "2048"))
        self.temperature = float(os.getenv("AI_TEMPERATURE", "0.95")) # æé«˜åˆ›é€ æ€§
        
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url="https://api.siliconflow.cn/v1"
        )
        
        logger.success(f"âœ… [Config] Model: {self.model} | Temp: {self.temperature}")

    def _detect_burst_mode(self, text: str) -> tuple[bool, int]:
        """
        æ£€æµ‹è¿å‘æ¶ˆæ¯æ¨¡å¼
        Returns: (is_burst, pressure_level)
        """
        lines = text.strip().split('\n')
        line_count = len(lines)
        
        # è®¡ç®—çŸ­æ¶ˆæ¯å æ¯”ï¼ˆ<=5å­—ç¬¦çš„è¡Œï¼‰
        short_lines = sum(1 for line in lines if len(line.strip()) <= 5)
        
        is_burst = line_count >= 3 or (line_count >= 2 and short_lines >= 2)
        pressure_level = min(line_count, 5)  # æœ€é«˜5çº§
        
        return is_burst, pressure_level

    def _parse_response(self, raw_content: str) -> Dict[str, Any]:
        """
        è§£æ LLM è¿”å›çš„ JSON å“åº”å¹¶éªŒè¯æ•°æ®ç»“æ„
        """
        clean_content = raw_content.replace("```json", "").replace("```", "").strip()
        
        logger.debug(f"ğŸ“ [Parse] Raw content length: {len(clean_content)}")
        
        try:
            data = json.loads(clean_content)
            # ä½¿ç”¨æ–°ç‰ˆæ¨¡å‹éªŒè¯
            validated = AdvisorResponse(**data)
            logger.debug(f"âœ… [Validate] Analysis: {validated.analysis[:20]}...")
            return validated.model_dump()
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [Parse] JSON Error: {e}")
            raise e
        except ValidationError as e:
            logger.error(f"âŒ [Parse] Schema Error: {e}")
            raise e

    def _parse_analysis_response(self, raw_content: str) -> Dict[str, Any]:
        """
        è§£ææ€åŠ¿æ„ŸçŸ¥å“åº” (SituationAnalysis)
        """
        clean_content = raw_content.replace("```json", "").replace("```", "").strip()
        
        try:
            data = json.loads(clean_content)
            validated = SituationAnalysis(**data)
            return validated.model_dump()
        except json.JSONDecodeError as e:
            logger.error(f"âŒ [Analyze Parse] JSON Error: {e}")
            raise e
        except ValidationError as e:
            logger.error(f"âŒ [Analyze Parse] Schema Error: {e}")
            raise e

    def _build_context_prompt(self, user_input: str, history: list, selected_styles: list) -> str:
        """
        æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„ Prompt
        
        Args:
            user_input: å¯¹æ–¹æœ€æ–°æ¶ˆæ¯
            history: å†å²å¯¹è¯è®°å½• [{"role": "user", "content": "..."}, ...]
            selected_styles: å·²é€‰æ‹©çš„3ç§é£æ ¼
            
        Returns:
            å®Œæ•´çš„ system prompt
        """
        # 1. æ ¼å¼åŒ–å†å²è®°å½•
        context_str = ""
        if history:
            context_str = "\n# ğŸ“œ Conversation History (Recent Context)\n"
            context_str += "ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç”¨äºç†è§£å½“å‰å±€åŠ¿çš„èƒŒæ™¯ï¼š\n"
            for i, msg in enumerate(history, 1):
                role = "å¯¹æ–¹" if msg.get("role") == "user" else "ä½ ä¹‹å‰çš„å»ºè®®"
                content = msg.get("content", "")
                context_str += f"{i}. {role}: {content}\n"
            context_str += "\n---\n"
        
        # 2. è·å–åŸºç¡€ prompt
        base_prompt = build_advisor_prompt(user_input, selected_styles)
        
        # 3. å°† context æ’å…¥åˆ° Input ä¹‹å‰
        if context_str:
            final_prompt = base_prompt.replace(
                "# Input - The Other Person's Message", 
                f"{context_str}# Input - The Other Person's Message (æœ€æ–°æ¶ˆæ¯)"
            )
        else:
            final_prompt = base_prompt
            
        return final_prompt

    # ==================== v8.0 æ–°å¢ï¼šåŒé˜¶æ®µ API ====================
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(2),
        retry=retry_if_exception_type((json.JSONDecodeError, ValidationError, Exception)),
        reraise=True,
    )
    async def analyze_situation(self, user_input: str, history: list = []) -> Dict[str, Any]:
        """
        v8.0 Phase 1: æ€åŠ¿æ„ŸçŸ¥ (Situation Awareness)
        åˆ†æå¯¹æ–¹æƒ…ç»ªã€æ„å›¾å’Œè¯­å¢ƒå‹è¿«æ„Ÿ
        
        Args:
            user_input: å¯¹æ–¹å‘æ¥çš„æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šè¡Œè¿å‘ï¼‰
            history: å†å²å¯¹è¯ä¸Šä¸‹æ–‡
            
        Returns:
            SituationAnalysis çš„å­—å…¸å½¢å¼
        """
        self._refresh_config()
        
        # 1. é¢„æ£€æµ‹è¿å‘æ¨¡å¼
        is_burst, pressure_level = self._detect_burst_mode(user_input)
        logger.info(f"ğŸ¯ [Analyze] Input: {user_input[:30]}... | Burst: {is_burst} | Pressure: {pressure_level}")
        
        # 2. æ„å»ºåˆ†æ Prompt
        system_prompt = build_analyze_prompt(user_input, history)
        
        try:
            # 3. è°ƒç”¨ LLM è¿›è¡Œå¿ƒç†ä¾§å†™
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹æ¶ˆæ¯ï¼š\n{user_input}"},
                ],
                temperature=0.7,  # åˆ†æé˜¶æ®µé™ä½éšæœºæ€§
                max_tokens=512,   # åˆ†æè¾“å‡ºè¾ƒçŸ­
                response_format={"type": "json_object"},
            )
            
            raw_content = response.choices[0].message.content
            result = self._parse_analysis_response(raw_content)
            
            # 4. ç”¨é¢„æ£€æµ‹ç»“æœè¦†ç›–ï¼ˆæ›´å‡†ç¡®ï¼‰
            result["burst_detected"] = is_burst
            result["pressure_level"] = max(result.get("pressure_level", 0), pressure_level)
            
            logger.success(f"âœ… [Analyze] Strategy: {result.get('strategy')} | Emotion: {result.get('emotion_score')}")
            
            return result
            
        except Exception as exc:
            logger.error(f"âŒ [Analyze] Failed: {exc}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return {
                "summary": "æ— æ³•å®Œæˆæ€åŠ¿åˆ†æï¼Œè¯·æ‰‹åŠ¨è°ƒæ•´å‚æ•°ã€‚",
                "emotion_score": 0,
                "intent": "UNKNOWN",
                "strategy": "COMFORT",
                "confidence": 0.5,
                "burst_detected": is_burst,
                "pressure_level": pressure_level
            }
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(2),
        retry=retry_if_exception_type((json.JSONDecodeError, ValidationError, Exception)),
        reraise=True,
    )
    async def execute_tactics(
        self, 
        user_input: str, 
        analysis: Dict[str, Any], 
        history: list = []
    ) -> Dict[str, Any]:
        """
        v8.0 Phase 2: æˆ˜æœ¯æ‰§è¡Œ (Tactical Execution)
        åŸºäºç¡®å®šçš„æˆ˜æœ¯ç­–ç•¥ç”Ÿæˆå›å¤é€‰é¡¹
        
        Args:
            user_input: å¯¹æ–¹åŸå§‹æ¶ˆæ¯
            analysis: ç»ç”¨æˆ·ç¡®è®¤/ä¿®æ”¹çš„æˆ˜æœ¯åˆ†æ (SituationAnalysis)
            history: å†å²å¯¹è¯ä¸Šä¸‹æ–‡
            
        Returns:
            AdvisorResponse çš„å­—å…¸å½¢å¼ (analysis, options)
        """
        self._refresh_config()
        
        # 1. éšæœºæŠ½å–é£æ ¼
        selected_styles = get_random_styles(3)
        style_names = [s['name'] for s in selected_styles]
        logger.info(f"ğŸ² [Execute] Styles: {style_names} | Strategy: {analysis.get('strategy')}")
        
        # 2. æ„å»ºæ‰§è¡Œ Prompt
        system_prompt = build_execute_prompt(user_input, analysis, selected_styles, history)
        
        try:
            # 3. è°ƒç”¨ LLM ç”Ÿæˆå›å¤
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"åŸºäº{analysis.get('strategy')}ç­–ç•¥ï¼Œä¸ºä»¥ä¸‹æ¶ˆæ¯ç”Ÿæˆ3ä¸ªå›å¤é€‰é¡¹ï¼š\n{user_input}"},
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"},
            )
            
            raw_content = response.choices[0].message.content
            result = self._parse_response(raw_content)
            
            logger.success(f"âœ… [Execute] Generated {len(result.get('options', []))} options")
            
            return result
            
        except Exception as exc:
            logger.error(f"âŒ [Execute] Failed: {exc}")
            raise exc

    # ==================== åŸæœ‰æ¥å£ï¼ˆä¿æŒå…¼å®¹ï¼‰ ====================

    # v8.1: æˆ˜æœ¯æ„å›¾åˆ°ç­–ç•¥çš„æ˜ å°„
    INTENT_TO_STRATEGY = {
        "PRESSURE": "OFFENSIVE_FLIRT",   # é«˜å‹å¨æ…‘ â†’ è¿›æ”»è°ƒæƒ…
        "LURE": "DEFENSIVE_FLIRT",       # ç¤ºå¼±è¯±æ•Œ â†’ é˜²å®ˆè°ƒæƒ…
        "PROBE": "PUSH_PULL",            # æ¨¡ç³Šè¯•æ¢ â†’ æ¨æ‹‰æˆ˜æœ¯
        "COMFORT": "COMFORT",            # æƒ…ç»ªå®‰æŠš â†’ å®‰æŠš
    }

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(2),
        retry=retry_if_exception_type((json.JSONDecodeError, ValidationError, Exception)),
        reraise=True,
    )
    async def generate_response_with_intent(
        self, 
        user_input: str, 
        history: list = [], 
        tactical_intent: str = None
    ) -> Dict[str, Any]:
        """
        v8.1ã€Œç›´å‡º+çƒ­ä¿®ã€æ¨¡å¼çš„ç”Ÿæˆæ¥å£
        
        Args:
            user_input: å¯¹æ–¹å‘æ¥çš„æ–‡æœ¬
            history: å†å²å¯¹è¯è®°å½•
            tactical_intent: ç”¨æˆ·æŒ‡å®šçš„æˆ˜æœ¯æ„å›¾ (PRESSURE/LURE/PROBE/COMFORT)
            
        Returns:
            AdvisorResponse çš„å­—å…¸å½¢å¼ (analysis, options)
        """
        self._refresh_config()
        
        # 1. éšæœºæŠ½å– 3 ç§é£æ ¼
        selected_styles = get_random_styles(3)
        style_names = [s['name'] for s in selected_styles]
        
        intent_str = f" | Intent: {tactical_intent}" if tactical_intent else " | Auto"
        logger.info(f"ğŸ² [Generate] Styles: {style_names} | History: {len(history)}{intent_str}")
        
        # 2. æ„å»ºå¸¦è®°å¿†çš„ Prompt
        system_prompt = self._build_context_prompt(user_input, history, selected_styles)
        
        # 3. å¦‚æœæœ‰æˆ˜æœ¯æ„å›¾ï¼Œæ·»åŠ æˆ˜æœ¯æŒ‡ä»¤
        if tactical_intent and tactical_intent in self.INTENT_TO_STRATEGY:
            strategy = self.INTENT_TO_STRATEGY[tactical_intent]
            intent_instructions = f"""

# ğŸ¯ ç”¨æˆ·æŒ‡å®šæˆ˜æœ¯æ„å›¾: {tactical_intent}
ç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨ã€Œ{tactical_intent}ã€ç­–ç•¥ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹é£æ ¼æ–¹å‘ç”Ÿæˆå›å¤ï¼š

- PRESSURE (é«˜å‹å¨æ…‘): å›å¤è¦å¼ºåŠ¿ã€ä¸»å¯¼ã€å¸¦æœ‰è½»å¾®å‹è¿«æ„Ÿï¼Œè®©å¯¹æ–¹æ„Ÿå—åˆ°ä½ çš„æ°”åœº
- LURE (ç¤ºå¼±è¯±æ•Œ): å›å¤è¦æ’’å¨‡ã€ç¤ºå¼±ã€å–èŒï¼Œå¼•å‘å¯¹æ–¹çš„ä¿æŠ¤æ¬²å’Œå¿ƒè½¯
- PROBE (æ¨¡ç³Šè¯•æ¢): å›å¤è¦å«ç³Šã€è¯é‡Œæœ‰è¯ã€ä¸æ­£é¢å›åº”ï¼Œè®©å¯¹æ–¹çŒœæµ‹ä½ çš„çœŸå®æ„å›¾
- COMFORT (æƒ…ç»ªå®‰æŠš): å›å¤è¦å…±æƒ…ã€ç†è§£ã€æ¸©æŸ”é™ªä¼´ï¼Œè®©å¯¹æ–¹æ„Ÿå—åˆ°è¢«æ¥çº³å’Œæ”¯æŒ

å½“å‰ç­–ç•¥: {tactical_intent}
æ‰€æœ‰3ä¸ªé€‰é¡¹éƒ½åº”è¯¥ç¬¦åˆè¿™ä¸ªæˆ˜æœ¯æ–¹å‘ï¼Œä½†ä¿æŒé£æ ¼å·®å¼‚ã€‚
"""
            system_prompt += intent_instructions
        
        logger.info(f"âš¡ [Request] Input: {user_input[:30]}...")

        try:
            # 4. è°ƒç”¨ LLM
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"å¯¹æ–¹æœ€æ–°æ¶ˆæ¯ï¼š{user_input}"},
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"},
            )

            raw_content = response.choices[0].message.content
            
            # 5. è§£æç»“æœ
            result = self._parse_response(raw_content)
            
            logger.success(f"âœ… [LLM] Generation successful | Options: {len(result.get('options', []))}")
            
            return result
            
        except Exception as exc:
            logger.error(f"âŒ [LLM] Failed: {exc}")
            raise exc

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(2),
        retry=retry_if_exception_type((json.JSONDecodeError, ValidationError, Exception)),
        reraise=True,
    )
    async def generate_response(self, user_input: str, history: list = []) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ‹çˆ±å†›å¸ˆå»ºè®®ï¼ˆæ”¯æŒå†å²ä¸Šä¸‹æ–‡ï¼‰
        
        Args:
            user_input: å¯¹æ–¹å‘æ¥çš„æ–‡æœ¬
            history: å†å²å¯¹è¯è®°å½•ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£
            
        Returns:
            AdvisorResponse çš„å­—å…¸å½¢å¼ (analysis, options)
        """
        self._refresh_config()
        
        # 1. éšæœºæŠ½å– 3 ç§é£æ ¼
        selected_styles = get_random_styles(3)
        style_names = [s['name'] for s in selected_styles]
        logger.info(f"ğŸ² [Random] Styles: {style_names} | History Depth: {len(history)}")
        
        # 2. æ„å»ºå¸¦è®°å¿†çš„ Prompt
        system_prompt = self._build_context_prompt(user_input, history, selected_styles)
        
        logger.info(f"âš¡ [Request] Input: {user_input[:30]}... | Context: {len(history)} messages")

        try:
            # 3. è°ƒç”¨ LLM
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    # ä¹Ÿå¯ä»¥é€‰æ‹©æŠŠ user_input æ”¾åœ¨è¿™é‡Œå†æ¬¡å¼ºè°ƒï¼Œæˆ–è€…ä»…é  system prompt
                    {"role": "user", "content": f"å¯¹æ–¹æœ€æ–°æ¶ˆæ¯ï¼š{user_input}"},
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"},
            )

            raw_content = response.choices[0].message.content
            
            # 4. è§£æç»“æœ
            result = self._parse_response(raw_content)
            
            logger.success(f"âœ… [LLM] Generation successful | Options: {len(result.get('options', []))}")
            
            return result
            
        except Exception as exc:
            logger.error(f"âŒ [LLM] Failed: {exc}")
            raise exc

# åˆ›å»ºå…¨å±€å®ä¾‹
ai_service = AIService()