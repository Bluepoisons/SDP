"""
AI Service - æ ¸å¿ƒ LLM è°ƒç”¨æœåŠ¡
ä½¿ç”¨ AsyncOpenAI åŸç”Ÿå¼‚æ­¥å®¢æˆ·ç«¯ + Loguru ç»“æ„åŒ–æ—¥å¿—
"""
import json
import os
from typing import Any, Dict

from dotenv import load_dotenv
from loguru import logger
from openai import AsyncOpenAI
from pydantic import ValidationError
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed

from models.schemas import GameResponse
from config.styles import build_system_prompt

class AIService:
    """
    AI æœåŠ¡ç±» - è´Ÿè´£ä¸ LLM äº¤äº’
    
    Features:
    - AsyncOpenAI åŸç”Ÿå¼‚æ­¥å®¢æˆ·ç«¯ï¼ˆé«˜å¹¶å‘æ€§èƒ½ä¼˜åŒ–ï¼‰
    - Tenacity è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼ˆå®¹é”™æ€§ï¼‰
    - Loguru ç»“æ„åŒ–æ—¥å¿—ï¼ˆå¯è§‚æµ‹æ€§ï¼‰
    - Pydantic æ•°æ®æ ¡éªŒï¼ˆç±»å‹å®‰å…¨ï¼‰
    """
    
    def __init__(self) -> None:
        logger.info("ğŸš€ [AIService] Initializing AI Service...")
        self._refresh_config()

    def _refresh_config(self) -> None:
        """åŠ è½½ç¯å¢ƒå˜é‡é…ç½®"""
        load_dotenv(override=True)
        self.api_key = os.getenv("SILICONFLOW_API_KEY", "")
        self.model = os.getenv("AI_MODEL", "Qwen/Qwen2.5-72B-Instruct")
        self.max_tokens = int(os.getenv("AI_MAX_TOKENS", "2048"))
        self.temperature = float(os.getenv("AI_TEMPERATURE", "0.85"))
        
        # ä½¿ç”¨ AsyncOpenAI åŸç”Ÿå¼‚æ­¥å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url="https://api.siliconflow.cn/v1"
        )
        
        logger.success(f"âœ… [Config] Model: {self.model} | Tokens: {self.max_tokens} | Temp: {self.temperature}")

    def _parse_response(self, raw_content: str) -> Dict[str, Any]:
        """
        è§£æ LLM è¿”å›çš„ JSON å“åº”å¹¶éªŒè¯æ•°æ®ç»“æ„
        
        Args:
            raw_content: LLM è¿”å›çš„åŸå§‹å­—ç¬¦ä¸²
            
        Returns:
            éªŒè¯åçš„å­—å…¸æ•°æ®
            
        Raises:
            json.JSONDecodeError: JSON è§£æå¤±è´¥
            ValidationError: Pydantic æ•°æ®éªŒè¯å¤±è´¥
        """
        # æ¸…ç†å¯èƒ½çš„ Markdown ä»£ç å—æ ‡è®°
        clean_content = raw_content.replace("```json", "").replace("```", "").strip()
        
        logger.debug(f"ğŸ“ [Parse] Raw content length: {len(clean_content)}")
        
        # JSON è§£æ
        data = json.loads(clean_content)
        
        # Pydantic éªŒè¯
        validated = GameResponse(**data)
        
        logger.debug(f"âœ… [Validate] Mood: {validated.mood} | Scene: {validated.scene}")
        
        return validated.model_dump()

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(2),
        retry=retry_if_exception_type((json.JSONDecodeError, ValidationError, Exception)),
        reraise=True,
    )
    async def generate_response(self, user_input: str, style: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆ AI å“åº” - ä½¿ç”¨åŸç”Ÿå¼‚æ­¥å®¢æˆ·ç«¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            style: é£æ ¼ä»£ç  (TSUNDERE/YANDERE/KUUDERE/GENKI)
            
        Returns:
            åŒ…å« summary, text, mood, scene, options çš„å­—å…¸
            
        Raises:
            Exception: API è°ƒç”¨å¤±è´¥æˆ–æ•°æ®éªŒè¯å¤±è´¥
        """
        self._refresh_config()
        
        # ä»é…ç½®æ¨¡å—åŠ¨æ€æ„å»º Prompt
        system_prompt = build_system_prompt(style)
        
        logger.info(f"âš¡ [Request] Style: {style} | Input: {user_input[:30]}...")

        try:
            # ç›´æ¥ await åŸç”Ÿå¼‚æ­¥æ–¹æ³•ï¼Œæ— éœ€ asyncio.to_thread
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input},
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"},
            )

            raw_content = response.choices[0].message.content
            result = self._parse_response(raw_content)
            
            logger.success(f"âœ… [LLM] Generation successful | Options: {len(result.get('options', []))}")
            
            return result
            
        except Exception as exc:
            logger.error(f"âŒ [LLM] Failed: {exc}")
            raise exc


# åˆ›å»ºå…¨å±€å®ä¾‹ä¾› main.py å¯¼å…¥
ai_service = AIService()